{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d8692f",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "### Deteccion de anomalias usando autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2745ae",
   "metadata": {},
   "source": [
    "Anomaly Detection using AutoEncoders\n",
    "\n",
    "AutoEncoders are widely used in anomaly detection. _The reconstruction errors are used as the anomaly scores_. Let us look at how we can use AutoEncoder for anomaly detection using TensorFlow.\n",
    "\n",
    "Import the required libraries and load the data. Here we are using the ECG data which consists of labels 0 and 1. Label 0 denotes the observation as an anomaly and label 1 denotes the observation as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ccd348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:47:19.418472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-26 10:47:19.799615: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-26 10:47:20.536410: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-26 10:47:20.536469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-26 10:47:20.536474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.112522</td>\n",
       "      <td>-2.827204</td>\n",
       "      <td>-3.773897</td>\n",
       "      <td>-4.349751</td>\n",
       "      <td>-4.376041</td>\n",
       "      <td>-3.474986</td>\n",
       "      <td>-2.181408</td>\n",
       "      <td>-1.818286</td>\n",
       "      <td>-1.250522</td>\n",
       "      <td>-0.477492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792168</td>\n",
       "      <td>0.933541</td>\n",
       "      <td>0.796958</td>\n",
       "      <td>0.578621</td>\n",
       "      <td>0.257740</td>\n",
       "      <td>0.228077</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.925286</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.100878</td>\n",
       "      <td>-3.996840</td>\n",
       "      <td>-4.285843</td>\n",
       "      <td>-4.506579</td>\n",
       "      <td>-4.022377</td>\n",
       "      <td>-3.234368</td>\n",
       "      <td>-1.566126</td>\n",
       "      <td>-0.992258</td>\n",
       "      <td>-0.754680</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538356</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>0.787490</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.773820</td>\n",
       "      <td>1.119621</td>\n",
       "      <td>-1.436250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-2.593450</td>\n",
       "      <td>-3.874230</td>\n",
       "      <td>-4.584095</td>\n",
       "      <td>-4.187449</td>\n",
       "      <td>-3.151462</td>\n",
       "      <td>-1.742940</td>\n",
       "      <td>-1.490659</td>\n",
       "      <td>-1.183580</td>\n",
       "      <td>-0.394229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886073</td>\n",
       "      <td>0.531452</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.021919</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.532197</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-0.421797</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490473</td>\n",
       "      <td>-1.914407</td>\n",
       "      <td>-3.616364</td>\n",
       "      <td>-4.318823</td>\n",
       "      <td>-4.268016</td>\n",
       "      <td>-3.881110</td>\n",
       "      <td>-2.993280</td>\n",
       "      <td>-1.671131</td>\n",
       "      <td>-1.333884</td>\n",
       "      <td>-0.965629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350816</td>\n",
       "      <td>0.499111</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.842069</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>1.086798</td>\n",
       "      <td>1.403011</td>\n",
       "      <td>-0.383564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800232</td>\n",
       "      <td>-0.874252</td>\n",
       "      <td>-2.384761</td>\n",
       "      <td>-3.973292</td>\n",
       "      <td>-4.338224</td>\n",
       "      <td>-3.802422</td>\n",
       "      <td>-2.534510</td>\n",
       "      <td>-1.783423</td>\n",
       "      <td>-1.594450</td>\n",
       "      <td>-0.753199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148884</td>\n",
       "      <td>0.958434</td>\n",
       "      <td>1.059025</td>\n",
       "      <td>1.371682</td>\n",
       "      <td>1.277392</td>\n",
       "      <td>0.960304</td>\n",
       "      <td>0.971020</td>\n",
       "      <td>1.614392</td>\n",
       "      <td>1.421456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
       "1 -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
       "2 -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
       "3  0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
       "4  0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
       "\n",
       "        7         8         9    ...       131       132       133       134  \\\n",
       "0 -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958  0.578621   \n",
       "1 -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490  0.724046   \n",
       "2 -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377 -0.021919   \n",
       "3 -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345  0.842069   \n",
       "4 -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025  1.371682   \n",
       "\n",
       "        135       136       137       138       139  140  \n",
       "0  0.257740  0.228077  0.123431  0.925286  0.193137  1.0  \n",
       "1  0.555784  0.476333  0.773820  1.119621 -1.436250  1.0  \n",
       "2 -0.713683 -0.532197  0.321097  0.904227 -0.421797  1.0  \n",
       "3  0.952074  0.990133  1.086798  1.403011 -0.383564  1.0  \n",
       "4  1.277392  0.960304  0.971020  1.614392  1.421456  1.0  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "\n",
    "# Download the dataset\n",
    "PATH_TO_DATA = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'\n",
    "data = pd.read_csv(PATH_TO_DATA, header=None)\n",
    "data.head()\n",
    "\n",
    "# data shape\n",
    "# (4998, 141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbd341c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 141)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e9af54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2919\n",
       "0.0    2079\n",
       "Name: 140, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:,140].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656efe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last column is the target\n",
    "# 0 = anomaly, 1 = normal\n",
    "TARGET = 140\n",
    "\n",
    "features = data.drop(TARGET, axis=1)\n",
    "target = data[TARGET]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9949ee3",
   "metadata": {},
   "source": [
    "Como lo que nos interesa es detectar cuales son los novedosos, usamos solo los datos normales para entrenar (queremos provocar la diferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d9673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = y_train[y_train == 1].index\n",
    "train_data = x_train.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5450ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scale the input data\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
    "x_test_scaled = min_max_scaler.transform(x_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229100ca",
   "metadata": {},
   "source": [
    "The last column in the data is the target ( column name is 140). Split the data for training and testing and scale the data using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87da6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 124ms/step - loss: 0.0121 - mse: 0.0267 - val_loss: 0.0138 - val_mse: 0.0316\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0119 - mse: 0.0262 - val_loss: 0.0137 - val_mse: 0.0314\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0117 - mse: 0.0259 - val_loss: 0.0136 - val_mse: 0.0311\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0114 - mse: 0.0252 - val_loss: 0.0133 - val_mse: 0.0305\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0109 - mse: 0.0241 - val_loss: 0.0130 - val_mse: 0.0298\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0102 - mse: 0.0224 - val_loss: 0.0127 - val_mse: 0.0293\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0093 - mse: 0.0205 - val_loss: 0.0128 - val_mse: 0.0295\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0086 - mse: 0.0190 - val_loss: 0.0129 - val_mse: 0.0295\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0081 - mse: 0.0179 - val_loss: 0.0124 - val_mse: 0.0284\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0074 - mse: 0.0163 - val_loss: 0.0119 - val_mse: 0.0274\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0069 - mse: 0.0151 - val_loss: 0.0117 - val_mse: 0.0268\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0064 - mse: 0.0142 - val_loss: 0.0116 - val_mse: 0.0266\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0061 - mse: 0.0135 - val_loss: 0.0116 - val_mse: 0.0267\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0058 - mse: 0.0129 - val_loss: 0.0115 - val_mse: 0.0264\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0057 - mse: 0.0125 - val_loss: 0.0110 - val_mse: 0.0253\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0054 - mse: 0.0120 - val_loss: 0.0108 - val_mse: 0.0247\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0053 - mse: 0.0117 - val_loss: 0.0107 - val_mse: 0.0246\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0051 - mse: 0.0114 - val_loss: 0.0107 - val_mse: 0.0245\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0050 - mse: 0.0111 - val_loss: 0.0104 - val_mse: 0.0240\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0049 - mse: 0.0109 - val_loss: 0.0103 - val_mse: 0.0237\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0048 - mse: 0.0107 - val_loss: 0.0102 - val_mse: 0.0234\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0048 - mse: 0.0106 - val_loss: 0.0100 - val_mse: 0.0231\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0047 - mse: 0.0105 - val_loss: 0.0099 - val_mse: 0.0229\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0047 - mse: 0.0104 - val_loss: 0.0099 - val_mse: 0.0228\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0046 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0046 - mse: 0.0103 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0045 - mse: 0.0101 - val_loss: 0.0098 - val_mse: 0.0228\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0045 - mse: 0.0101 - val_loss: 0.0098 - val_mse: 0.0228\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0045 - mse: 0.0100 - val_loss: 0.0098 - val_mse: 0.0228\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0045 - mse: 0.0100 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0045 - mse: 0.0099 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0098 - val_mse: 0.0226\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0098 - val_mse: 0.0226\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0044 - mse: 0.0098 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0044 - mse: 0.0098 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0044 - mse: 0.0098 - val_loss: 0.0097 - val_mse: 0.0224\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0044 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0044 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0224\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0043 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0043 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0043 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0043 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0043 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0043 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0095 - val_mse: 0.0220\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0095 - val_mse: 0.0221\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0095 - val_mse: 0.0220\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0042 - mse: 0.0093 - val_loss: 0.0095 - val_mse: 0.0219\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0042 - mse: 0.0093 - val_loss: 0.0095 - val_mse: 0.0219\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0094 - val_mse: 0.0218\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0094 - val_mse: 0.0219\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0094 - val_mse: 0.0217\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0041 - mse: 0.0091 - val_loss: 0.0094 - val_mse: 0.0218\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0040 - mse: 0.0090 - val_loss: 0.0093 - val_mse: 0.0217\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0040 - mse: 0.0090 - val_loss: 0.0093 - val_mse: 0.0216\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0040 - mse: 0.0090 - val_loss: 0.0094 - val_mse: 0.0217\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0040 - mse: 0.0089 - val_loss: 0.0093 - val_mse: 0.0215\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0039 - mse: 0.0088 - val_loss: 0.0093 - val_mse: 0.0216\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0039 - mse: 0.0088 - val_loss: 0.0092 - val_mse: 0.0214\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0039 - mse: 0.0087 - val_loss: 0.0093 - val_mse: 0.0215\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0038 - mse: 0.0086 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0038 - mse: 0.0086 - val_loss: 0.0093 - val_mse: 0.0215\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0038 - mse: 0.0085 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0038 - mse: 0.0084 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0037 - mse: 0.0084 - val_loss: 0.0093 - val_mse: 0.0214\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0037 - mse: 0.0083 - val_loss: 0.0092 - val_mse: 0.0212\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0037 - mse: 0.0082 - val_loss: 0.0093 - val_mse: 0.0214\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0037 - mse: 0.0082 - val_loss: 0.0092 - val_mse: 0.0212\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0036 - mse: 0.0082 - val_loss: 0.0093 - val_mse: 0.0214\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0036 - mse: 0.0081 - val_loss: 0.0092 - val_mse: 0.0212\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0036 - mse: 0.0080 - val_loss: 0.0093 - val_mse: 0.0214\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0036 - mse: 0.0080 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0036 - mse: 0.0080 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0035 - mse: 0.0079 - val_loss: 0.0092 - val_mse: 0.0212\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0035 - mse: 0.0079 - val_loss: 0.0093 - val_mse: 0.0214\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0035 - mse: 0.0078 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0035 - mse: 0.0078 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0035 - mse: 0.0078 - val_loss: 0.0092 - val_mse: 0.0211\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0035 - mse: 0.0078 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0035 - mse: 0.0077 - val_loss: 0.0091 - val_mse: 0.0210\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0035 - mse: 0.0077 - val_loss: 0.0092 - val_mse: 0.0211\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0077 - val_loss: 0.0091 - val_mse: 0.0209\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0077 - val_loss: 0.0092 - val_mse: 0.0211\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0077 - val_loss: 0.0091 - val_mse: 0.0209\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0076 - val_loss: 0.0092 - val_mse: 0.0210\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0034 - mse: 0.0076 - val_loss: 0.0090 - val_mse: 0.0208\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0076 - val_loss: 0.0092 - val_mse: 0.0211\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0034 - mse: 0.0076 - val_loss: 0.0090 - val_mse: 0.0207\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0034 - mse: 0.0075 - val_loss: 0.0091 - val_mse: 0.0210\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0034 - mse: 0.0075 - val_loss: 0.0090 - val_mse: 0.0206\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0033 - mse: 0.0075 - val_loss: 0.0090 - val_mse: 0.0206\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0033 - mse: 0.0074 - val_loss: 0.0090 - val_mse: 0.0207\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0033 - mse: 0.0073 - val_loss: 0.0089 - val_mse: 0.0204\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0033 - mse: 0.0074 - val_loss: 0.0089 - val_mse: 0.0206\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0033 - mse: 0.0074 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0033 - mse: 0.0073 - val_loss: 0.0089 - val_mse: 0.0205\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0032 - mse: 0.0073 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0032 - mse: 0.0072 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0032 - mse: 0.0072 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0032 - mse: 0.0072 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0032 - mse: 0.0071 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0032 - mse: 0.0072 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0032 - mse: 0.0071 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0031 - mse: 0.0070 - val_loss: 0.0087 - val_mse: 0.0201\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0031 - mse: 0.0070 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0031 - mse: 0.0070 - val_loss: 0.0088 - val_mse: 0.0201\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0031 - mse: 0.0070 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0031 - mse: 0.0069 - val_loss: 0.0087 - val_mse: 0.0201\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0031 - mse: 0.0069 - val_loss: 0.0087 - val_mse: 0.0199\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0030 - mse: 0.0068 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0030 - mse: 0.0068 - val_loss: 0.0087 - val_mse: 0.0199\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0030 - mse: 0.0068 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0030 - mse: 0.0068 - val_loss: 0.0087 - val_mse: 0.0199\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0030 - mse: 0.0067 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0030 - mse: 0.0067 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0030 - mse: 0.0067 - val_loss: 0.0088 - val_mse: 0.0201\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0030 - mse: 0.0066 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0066 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0066 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0029 - mse: 0.0065 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0065 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0065 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0029 - mse: 0.0065 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0065 - val_loss: 0.0089 - val_mse: 0.0204\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0064 - val_loss: 0.0090 - val_mse: 0.0206\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0029 - mse: 0.0064 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0029 - mse: 0.0064 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0089 - val_mse: 0.0206\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0087 - val_mse: 0.0199\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0062 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0028 - mse: 0.0062 - val_loss: 0.0088 - val_mse: 0.0201\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0063 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0028 - mse: 0.0062 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0028 - mse: 0.0062 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0061 - val_loss: 0.0088 - val_mse: 0.0201\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0061 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0062 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0028 - mse: 0.0061 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0027 - mse: 0.0061 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0061 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0061 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0061 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0084 - val_mse: 0.0194\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0087 - val_mse: 0.0199\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0027 - mse: 0.0059 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0027 - mse: 0.0059 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0059 - val_loss: 0.0086 - val_mse: 0.0196\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0027 - mse: 0.0060 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0197\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0026 - mse: 0.0059 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0085 - val_mse: 0.0197\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0083 - val_mse: 0.0192\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0058 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 187/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0057 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0084 - val_mse: 0.0194\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0084 - val_mse: 0.0194\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0057 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0057 - val_loss: 0.0088 - val_mse: 0.0202\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0087 - val_mse: 0.0200\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0084 - val_mse: 0.0194\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0196\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0087 - val_mse: 0.0201\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0056 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0084 - val_mse: 0.0193\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0083 - val_mse: 0.0191\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0025 - mse: 0.0054 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0083 - val_mse: 0.0191\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0025 - mse: 0.0055 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0084 - val_mse: 0.0191\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0085 - val_mse: 0.0195\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0086 - val_mse: 0.0197\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0084 - val_mse: 0.0192\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0083 - val_mse: 0.0191\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0054 - val_loss: 0.0082 - val_mse: 0.0189\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0082 - val_mse: 0.0189\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0024 - mse: 0.0052 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0052 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0052 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0053 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0052 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0024 - mse: 0.0052 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0052 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0052 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 249/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0081 - val_mse: 0.0184\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0081 - val_mse: 0.0184\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0051 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0085 - val_mse: 0.0194\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0050 - val_loss: 0.0086 - val_mse: 0.0195\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0050 - val_loss: 0.0085 - val_mse: 0.0193\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0082 - val_mse: 0.0188\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0050 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0050 - val_loss: 0.0081 - val_mse: 0.0184\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0083 - val_mse: 0.0190\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0050 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0050 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0081 - val_mse: 0.0183\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0049 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0048 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0022 - mse: 0.0048 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0048 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0048 - val_loss: 0.0081 - val_mse: 0.0186\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0048 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0048 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0047 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0046 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0076 - val_mse: 0.0175\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0021 - mse: 0.0046 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0076 - val_mse: 0.0173\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0045 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 373/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0082 - val_mse: 0.0187\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0081 - val_mse: 0.0185\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0020 - mse: 0.0043 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0044 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0020 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0175\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0175\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0076 - val_mse: 0.0175\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0078 - val_mse: 0.0179\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0043 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0077 - val_mse: 0.0176\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0076 - val_mse: 0.0173\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0074 - val_mse: 0.0169\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0073 - val_mse: 0.0168\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0073 - val_mse: 0.0168\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0167\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0073 - val_mse: 0.0168\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0073 - val_mse: 0.0168\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0074 - val_mse: 0.0169\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0075 - val_mse: 0.0173\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 435/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0077 - val_mse: 0.0175\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0073 - val_mse: 0.0167\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0075 - val_mse: 0.0171\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0075 - val_mse: 0.0172\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0077 - val_mse: 0.0177\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0083 - val_mse: 0.0189\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0076 - val_mse: 0.0174\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0186\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0084 - val_mse: 0.0191\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0084 - val_mse: 0.0191\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0082 - val_mse: 0.0185\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0081 - val_mse: 0.0184\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0078 - val_mse: 0.0177\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0079 - val_mse: 0.0179\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0079 - val_mse: 0.0181\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0079 - val_mse: 0.0180\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0080 - val_mse: 0.0182\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0080 - val_mse: 0.0183\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0081 - val_mse: 0.0184\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0078 - val_mse: 0.0178\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0076 - val_mse: 0.0173\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0075 - val_mse: 0.0171\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0073 - val_mse: 0.0167\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0073 - val_mse: 0.0167\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0073 - val_mse: 0.0167\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0165\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0165\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0073 - val_mse: 0.0167\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0040 - val_loss: 0.0075 - val_mse: 0.0172\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0075 - val_mse: 0.0172\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0169\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0072 - val_mse: 0.0165\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0042 - val_loss: 0.0071 - val_mse: 0.0164\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0071 - val_mse: 0.0164\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0165\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0071 - val_mse: 0.0165\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0165\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0072 - val_mse: 0.0166\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0040 - val_loss: 0.0074 - val_mse: 0.0169\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0019 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0169\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0040 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0075 - val_mse: 0.0171\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0018 - mse: 0.0040 - val_loss: 0.0075 - val_mse: 0.0172\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0074 - val_mse: 0.0170\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0018 - mse: 0.0041 - val_loss: 0.0073 - val_mse: 0.0168\n"
     ]
    }
   ],
   "source": [
    "# create a model by subclassing Model class in tensorflow\n",
    "class AutoEncoder(Model):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  output_units: int\n",
    "    Number of output units\n",
    "  \n",
    "  code_size: int\n",
    "    Number of units in bottle neck\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, output_units, code_size=8):\n",
    "    super().__init__()\n",
    "    self.encoder = Sequential([\n",
    "      Dense(128, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(16, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(code_size, activation='relu')\n",
    "    ])\n",
    "    self.decoder = Sequential([\n",
    "      Dense(16, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(128, activation='relu'),\n",
    "      Dropout(0.1),\n",
    "      Dense(output_units, activation='sigmoid')\n",
    "    ])\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    encoded = self.encoder(inputs)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
    "# configurations of model\n",
    "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_scaled,\n",
    "    x_train_scaled,\n",
    "    epochs=500,\n",
    "    batch_size=2000,\n",
    "    validation_data=(x_test_scaled, x_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ace1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0049 - mse: 0.0108 - val_loss: 0.0101 - val_mse: 0.0233\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0104 - val_loss: 0.0101 - val_mse: 0.0232\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.0103 - val_loss: 0.0100 - val_mse: 0.0231\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0100 - val_mse: 0.0232\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.0100 - val_loss: 0.0100 - val_mse: 0.0231\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.0099 - val_loss: 0.0100 - val_mse: 0.0230\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0099 - val_mse: 0.0229\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0098 - val_loss: 0.0098 - val_mse: 0.0228\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0097 - val_loss: 0.0098 - val_mse: 0.0226\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0043 - mse: 0.0096 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0043 - mse: 0.0096 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0042 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0042 - mse: 0.0093 - val_loss: 0.0095 - val_mse: 0.0221\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0095 - val_mse: 0.0220\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0095 - val_mse: 0.0219\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0091 - val_loss: 0.0095 - val_mse: 0.0220\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0090 - val_loss: 0.0094 - val_mse: 0.0218\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0088 - val_loss: 0.0094 - val_mse: 0.0217\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0038 - mse: 0.0086 - val_loss: 0.0093 - val_mse: 0.0216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    x_train_scaled,\n",
    "    x_train_scaled,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test_scaled, x_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ac964",
   "metadata": {},
   "source": [
    "The encoder of the model consists of four layers that encode the data into lower dimensions. The decoder of the model consists of four layers that reconstruct the input data.\n",
    "\n",
    "The model is compiled with Mean Squared Logarithmic loss and Adam optimizer. The model is then trained with 20 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad491bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSLE Loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aae394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 870us/step\n",
      "Threshold: 0.004093334089714702\n",
      "32/32 [==============================] - 0s 810us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.967"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_threshold(model, x_train_scaled):\n",
    "    reconstructions = model.predict(x_train_scaled)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
    "    # threshold for anomaly scores\n",
    "    threshold = np.mean(reconstruction_errors.numpy()) \\\n",
    "    + np.std(reconstruction_errors.numpy())\n",
    "    return threshold\n",
    "\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "    # provides losses of individual instances\n",
    "    errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
    "    # 0 = anomaly, 1 = normal\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
    "    return preds\n",
    "\n",
    "threshold = find_threshold(model, x_train_scaled)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "# Threshold: 0.01001314025746261\n",
    "predictions = get_predictions(model, x_test_scaled, threshold)\n",
    "accuracy_score(predictions, y_test)\n",
    "\n",
    "# 0.944"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa19e9",
   "metadata": {},
   "source": [
    "The reconstruction errors are considered to be anomaly scores. The threshold is then calculated by summing the mean and standard deviation of the reconstruction errors. The reconstruction errors above this threshold are considered to be anomalies. We can further fine-tune the model by leveraging Keras-tuner.\n",
    "\n",
    "Â \n",
    "\n",
    "The autoencoder model does not have to symmetric encoder and decoder but the code size has to be smaller than that of the features in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ca654",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30f90fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import kerastuner.tuners as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32f9e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf9803bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a0392c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(california.data,columns = california.feature_names)\n",
    "target = pd.DataFrame(california.target,columns = california.target_names)\n",
    "\n",
    "x_train,x_test, y_train,_y_test = train_test_split(data,target,test_size=0.3,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4871a3",
   "metadata": {},
   "source": [
    "Scale the dataset using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c6f6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_datasets(x_train, x_test):\n",
    "    \"\"\"\n",
    "    Standard Scale test and train data\n",
    "    \"\"\"\n",
    "    standard_scaler = MinMaxScaler()\n",
    "    x_train_scaled = pd.DataFrame(\n",
    "        standard_scaler.fit_transform(x_train),\n",
    "        columns=x_train.columns\n",
    "    )\n",
    "    x_test_scaled = pd.DataFrame(\n",
    "        standard_scaler.transform(x_test),\n",
    "        columns = x_test.columns\n",
    "    )\n",
    "    return x_train_scaled, x_test_scaled\n",
    "  \n",
    "x_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744de78c",
   "metadata": {},
   "source": [
    "Train the autoencoder with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "033f1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "452/452 [==============================] - 2s 3ms/step - loss: 0.0871 - mae: 0.0871 - val_loss: 0.0287 - val_mae: 0.0287\n",
      "Epoch 2/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0277 - mae: 0.0277 - val_loss: 0.0261 - val_mae: 0.0261\n",
      "Epoch 3/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0251 - val_mae: 0.0251\n",
      "Epoch 4/15\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0181 - val_mae: 0.0181\n",
      "Epoch 5/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0163 - mae: 0.0163 - val_loss: 0.0153 - val_mae: 0.0153\n",
      "Epoch 6/15\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 0.0151 - mae: 0.0151 - val_loss: 0.0144 - val_mae: 0.0144\n",
      "Epoch 7/15\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 0.0145 - mae: 0.0145 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 8/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0142 - mae: 0.0142 - val_loss: 0.0141 - val_mae: 0.0141\n",
      "Epoch 9/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0140 - mae: 0.0140 - val_loss: 0.0137 - val_mae: 0.0137\n",
      "Epoch 10/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0136 - mae: 0.0136 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 11/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0134 - mae: 0.0134 - val_loss: 0.0130 - val_mae: 0.0130\n",
      "Epoch 12/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0133 - mae: 0.0133 - val_loss: 0.0133 - val_mae: 0.0133\n",
      "Epoch 13/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0130 - val_loss: 0.0125 - val_mae: 0.0125\n",
      "Epoch 14/15\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 0.0128 - mae: 0.0128 - val_loss: 0.0128 - val_mae: 0.0128\n",
      "Epoch 15/15\n",
      "452/452 [==============================] - 1s 3ms/step - loss: 0.0123 - mae: 0.0123 - val_loss: 0.0122 - val_mae: 0.0122\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoders(Model):\n",
    "    def __init__(self, output_units):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential(\n",
    "            [\n",
    "                Dense(64, activation=\"relu\"),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dense(7, activation=\"relu\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoder = Sequential(\n",
    "            [\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dense(output_units, activation=\"sigmoid\")\n",
    "            ]\n",
    "        )\n",
    "        ## AQUI HABIA UN ERROR EN EL PROGRAMA!!\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "auto_encoder = AutoEncoders(len(x_train_scaled.columns))\n",
    "\n",
    "auto_encoder.compile(\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "history = auto_encoder.fit(\n",
    "    x_train_scaled, \n",
    "    x_train_scaled, \n",
    "    epochs=15, \n",
    "    batch_size=32, \n",
    "    validation_data=(x_test_scaled, x_test_scaled)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa584a5",
   "metadata": {},
   "source": [
    "Here we have defined the autoencoder model by subclassing the Model class in Tensorflow and we compile the AutoEncoder model with mean absolute error and adam optimization function. We split the data into batches of 32 and we run it for 15 epochs.\n",
    "\n",
    "Get the encoder layer and use the method predict to reduce dimensions in data. Since we have seven hidden units in the bottleneck the data is reduced to seven features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71b44964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452/452 [==============================] - 0s 766us/step\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = auto_encoder.get_layer('sequential_14')\n",
    "reduced_df = pd.DataFrame(encoder_layer.predict(x_train_scaled))\n",
    "reduced_df = reduced_df.add_prefix('feature_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f400839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    14448\n",
       "Name: feature_6, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df['feature_6'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b693d2",
   "metadata": {},
   "source": [
    "In this way, AutoEncoders can be used to reduce dimensions in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d610837",
   "metadata": {},
   "source": [
    "`https://towardsdatascience.com/image-colorization-using-convolutional-autoencoders-fdabc1cb1dbe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd394a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
